# Data Pipeline Monitoring Automation Framework

A comprehensive AI-powered monitoring system for data pipelines across multiple platforms using Pydantic AI multi-agent architecture.

## ğŸ—ï¸ Architecture

This framework uses a multi-agent system to monitor data pipeline health across:
- **Airbyte** - Data connector sync jobs and streams
- **Databricks** - Job runs and cluster health
- **Power Automate** - Flow execution status  
- **Snowflake Tasks** - Task execution history

### Agent Architecture

```
ğŸ¯ Orchestrator Agent
â”œâ”€â”€ ğŸ“Š Airbyte Agent â†’ Airbyte API Tool
â”œâ”€â”€ âš™ï¸ Databricks Agent â†’ Databricks API Tool
â”œâ”€â”€ ğŸ”„ Power Automate Agent â†’ Microsoft Graph Tool
â”œâ”€â”€ â„ï¸ Snowflake Task Agent â†’ Snowflake Task API Tool
â”œâ”€â”€ ğŸ’¾ Snowflake DB Agent â†’ Database Operations Tool
â””â”€â”€ ğŸ“§ Email Agent â†’ Outlook API Tool
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Virtual environment (recommended)
- API access to monitored platforms
- Snowflake database for audit storage

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd audit-automation-framework
```

2. **Set up virtual environment**
```bash
python -m venv venv_linux
source venv_linux/bin/activate  # Linux/Mac
# or
venv_linux\Scripts\activate     # Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your API keys and configuration
```

### Configuration

#### Required Environment Variables

**LLM Configuration:**
```env
LLM_PROVIDER=openai
LLM_API_KEY=sk-your_openai_api_key_here
LLM_MODEL=gpt-4
```

**Platform API Keys:**
```env
# Airbyte (Choose one authentication method)
# Option 1: Static API Key (Legacy)
AIRBYTE_API_KEY=your_airbyte_access_token

# Option 2: OAuth2 Token Refresh (Recommended)
AIRBYTE_CLIENT_ID=your_airbyte_client_id
AIRBYTE_CLIENT_SECRET=your_airbyte_client_secret

# Common Airbyte Settings
AIRBYTE_WORKSPACE_ID=your_workspace_id

# Databricks  
DATABRICKS_API_KEY=your_databricks_token
DATABRICKS_BASE_URL=https://your-workspace.cloud.databricks.com

# Power Automate (Azure AD App)
POWER_AUTOMATE_CLIENT_ID=your_azure_app_client_id
POWER_AUTOMATE_CLIENT_SECRET=your_azure_app_secret
POWER_AUTOMATE_TENANT_ID=your_azure_tenant_id

# Snowflake
SNOWFLAKE_ACCOUNT=CURALEAF-CURAPROD.snowflakecomputing.com
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_DATABASE=DEV_POWERAPPS
SNOWFLAKE_SCHEMA=AUDIT_JOB_HUB

# Email Notifications (Azure AD App)
OUTLOOK_CLIENT_ID=your_outlook_client_id
OUTLOOK_CLIENT_SECRET=your_outlook_secret
OUTLOOK_TENANT_ID=your_tenant_id
```

## ğŸ–¥ï¸ Usage

### Interactive CLI

Launch the interactive monitoring interface:

```bash
python cli.py
```

**Available Commands:**
- `monitor all` - Comprehensive monitoring across all platforms
- `monitor airbyte` - Airbyte-specific monitoring
- `health check` - Quick system health assessment
- `help` - Show command help
- `config` - Display current configuration

### Automated Monitoring

Run automated monitoring cycles:

```bash
# Full monitoring cycle
python main.py --mode full

# Health check only
python main.py --mode health

# Custom notifications
python main.py --emails admin@company.com ops@company.com --from-email monitor@company.com

# Save results to file
python main.py --output-file results.json
```

### GitHub Actions Integration

The framework includes automated GitHub Actions workflows:

**Scheduled Monitoring:**
- Runs every 15 minutes
- Full platform monitoring
- Automatic notifications on issues
- Results stored as artifacts

**Manual Execution:**
- Trigger via GitHub Actions UI
- Customizable email recipients
- Configurable monitoring mode

## ğŸ”§ API Setup Instructions

### Airbyte Setup

**Option 1: OAuth2 Token Refresh (Recommended)**
1. Log into your Airbyte Cloud account
2. Go to Settings â†’ Applications
3. Click "Create Application"
4. Note down the Client ID and Client Secret
5. Set `AIRBYTE_CLIENT_ID` and `AIRBYTE_CLIENT_SECRET`

**Option 2: Static API Key (Legacy)**
1. Log into your Airbyte instance
2. Go to Settings â†’ Account â†’ Applications  
3. Click "Create Access Token"
4. Copy token and set as `AIRBYTE_API_KEY`

**Note:** OAuth2 is recommended as it provides automatic token refresh and better security.

### Databricks Setup
1. Log into your Databricks workspace
2. User Settings â†’ Access Tokens
3. Generate New Token
4. Copy token and set as `DATABRICKS_API_KEY`

### Power Automate Setup
1. Azure Portal â†’ App registrations
2. New registration
3. API permissions: Microsoft Graph
   - `Flow.Read.All`
   - `Directory.Read.All`
4. Generate client secret

### Snowflake Setup
1. Create database: `DEV_POWERAPPS`
2. Create schema: `AUDIT_JOB_HUB`
3. Grant permissions for INSERT/UPDATE operations
4. Tables are auto-created on first run

### Outlook Setup
1. Azure Portal â†’ App registrations
2. API permissions: Microsoft Graph
   - `Mail.Send`
   - `User.Read.All`
3. Generate client secret

## ğŸ“Š Data Storage

### Snowflake Schema

The framework automatically creates these tables in `DEV_POWERAPPS.AUDIT_JOB_HUB`:

**JOB_STATUS_RECORDS**
- Individual job execution records
- Platform-specific metadata
- Error messages and durations

**MONITORING_SESSIONS**
- Complete monitoring cycle results
- Overall health assessments
- Platform summaries

**PLATFORM_HEALTH_SUMMARIES**
- Per-platform health aggregations
- Success/failure rates
- Issue tracking

## ğŸ”” Notifications

### Email Templates

The system includes three notification templates:

1. **Critical Alert** - Multiple failures, system-wide issues
2. **Warning Alert** - Platform-specific failures
3. **Info Summary** - Regular status reports

### Notification Logic

- **CRITICAL**: >10 failed jobs or multiple platform failures
- **HIGH**: >5 failed jobs or single platform major issues  
- **NORMAL**: Minor issues, regular reports
- **LOW**: Informational updates only

## ğŸ§ª Testing

### Unit Tests
```bash
# Run all tests
pytest tests/ -v

# Coverage report
pytest tests/ --cov=agents --cov=tools --cov-report=html
```

### Integration Tests
```bash
# Test with actual APIs (requires valid .env)
python main.py --mode health

# Test CLI interface
python cli.py

# Test Airbyte token refresh functionality
python examples/test_token_refresh.py
```

### Agent Testing with TestModel
```python
from pydantic_ai.models.test import TestModel
from agents.airbyte_agent import airbyte_agent

# Fast agent validation without API calls
test_model = TestModel()
with airbyte_agent.override(model=test_model):
    result = await airbyte_agent.run("Test monitoring")
```

## ğŸ” Validation Commands

Before deployment, run these validation checks:

```bash
# Code quality
ruff check . --fix
mypy .

# Unit tests
pytest tests/ -v

# Manual system test
python main.py --mode health --output-file health.json
```

## ğŸ¢ Project Structure

```
audit-automation-framework/
â”œâ”€â”€ agents/                    # Pydantic AI agents
â”‚   â”œâ”€â”€ orchestrator_agent.py # Main coordinator
â”‚   â”œâ”€â”€ airbyte_agent.py      # Airbyte monitoring
â”‚   â”œâ”€â”€ email_agent.py        # Notifications
â”‚   â””â”€â”€ dependencies.py       # Shared dependencies
â”œâ”€â”€ tools/                     # Platform API integrations
â”‚   â”œâ”€â”€ airbyte_api.py        # Airbyte client
â”‚   â”œâ”€â”€ databricks_api.py     # Databricks client
â”‚   â”œâ”€â”€ snowflake_db_api.py   # Database operations
â”‚   â””â”€â”€ outlook_api.py        # Email client
â”œâ”€â”€ models/                    # Data models
â”‚   â”œâ”€â”€ job_status.py         # Core job models
â”‚   â”œâ”€â”€ platform_models.py    # Platform-specific models
â”‚   â””â”€â”€ notification_models.py # Email models
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py           # Configuration management
â”œâ”€â”€ .github/workflows/        # GitHub Actions
â”œâ”€â”€ cli.py                    # Interactive interface
â”œâ”€â”€ main.py                   # Automated execution
â””â”€â”€ requirements.txt          # Dependencies
```

## ğŸ”’ Security Best Practices

- Store all API keys in environment variables
- Use Azure AD app registrations with least-privilege permissions
- Enable MFA on all service accounts
- Rotate API keys regularly
- Monitor access logs for unusual activity
- Use Snowflake key-pair authentication in production

## ğŸ“ˆ Monitoring & Observability

### Logs
- Application logs: `monitoring.log`
- Structured logging with timestamps
- Error tracking and debugging information

### Metrics
- Job success/failure rates
- Platform availability percentages
- Response times and performance metrics
- Historical trend analysis

### Alerts
- Real-time failure notifications
- Escalation based on severity
- Integration with existing alerting systems

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Authentication Errors:**
- Verify API keys in .env file
- Check Azure AD app permissions
- Confirm service account access

**Connection Timeouts:**
- Increase timeout settings
- Check network connectivity
- Verify API endpoint URLs

**Missing Dependencies:**
- Run `pip install -r requirements.txt`
- Check Python version compatibility
- Verify virtual environment activation

### Debug Mode
```bash
# Enable debug logging
export DEBUG=true
export LOG_LEVEL=DEBUG
python main.py --mode health
```

## ğŸ“ Contributing

1. Follow PEP8 coding standards
2. Add unit tests for new features
3. Update documentation
4. Use conventional commit messages
5. Test with all supported platforms

## ğŸ“œ License

This project is licensed under the MIT License. See LICENSE file for details.

---

## ğŸ†˜ Support

For support and questions:
- Review the troubleshooting guide
- Check application logs
- Verify API configurations
- Contact the data engineering team

**Monitoring Dashboard:** Access real-time status via the CLI interface  
**Documentation:** Complete API setup guides in `.env.example`  
**Status:** Production-ready multi-agent monitoring system